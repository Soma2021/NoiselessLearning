{
  "metadata": {
    "anaconda-cloud": {},
    "celltoolbar": "Edit Metadata",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Learning an approximate model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "collapsed": false,
        "hide": true,
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import matplotlib as mpl\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "collapsed": false,
        "hide": true,
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "def make_simple_plot():\n",
        "    fig, axes=plt.subplots(figsize=(12,5), nrows=1, ncols=2);\n",
        "    axes[0].set_ylabel(\"$y$\")\n",
        "    axes[0].set_xlabel(\"$x$\")\n",
        "    axes[1].set_xlabel(\"$x$\")\n",
        "    axes[1].set_yticklabels([])\n",
        "    axes[0].set_ylim([-2,2])\n",
        "    axes[1].set_ylim([-2,2])\n",
        "    plt.tight_layout();\n",
        "    return axes\n",
        "def make_plot():\n",
        "    fig, axes=plt.subplots(figsize=(20,8), nrows=1, ncols=2);\n",
        "    axes[0].set_ylabel(\"$p_R$\")\n",
        "    axes[0].set_xlabel(\"$x$\")\n",
        "    axes[1].set_xlabel(\"$x$\")\n",
        "    axes[1].set_yticklabels([])\n",
        "    axes[0].set_ylim([0,1])\n",
        "    axes[1].set_ylim([0,1])\n",
        "    axes[0].set_xlim([0,1])\n",
        "    axes[1].set_xlim([0,1])\n",
        "    plt.tight_layout();\n",
        "    return axes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The impact of sampling\n",
        "\n",
        "How do a model's predictions vary with sample size? This is something we can learn very easily by looking at our toy model with the sine curve."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize = (9, 6))\n",
        "xgrid = xs=np.arange(-1.,1.,0.02)\n",
        "print(xgrid.shape)\n",
        "f = lambda x: np.sin(np.pi*x)\n",
        "fgrid = f(xgrid)\n",
        "plt.plot(xgrid, f(xgrid), '.');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "colors = mpl.rcParams['axes.prop_cycle'].by_key()['color']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "Xgrid = xgrid.reshape(-1,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "choices5=np.empty((200,5), dtype=\"int\")\n",
        "choices2=np.empty((200,2), dtype=\"int\")\n",
        "\n",
        "for i in range(200):\n",
        "    choices5[i,:] = np.random.choice(xgrid.shape[0], replace=False, size=5)\n",
        "    choices2[i,:] = np.random.choice(xgrid.shape[0], replace=False, size=2)\n",
        "axes=make_simple_plot()\n",
        "c=colors[1]\n",
        "axes[0].plot(xgrid, fgrid, alpha=0.9, lw=3)\n",
        "axes[1].plot(xgrid, fgrid, alpha=0.9, lw=3)\n",
        "for i in range(200):\n",
        "    l5 = LinearRegression()\n",
        "    l2 = LinearRegression()\n",
        "    p5=l5.fit(xgrid[choices5[i,:]].reshape(-1,1), fgrid[choices5[i,:]]).predict(Xgrid)\n",
        "    p2=l2.fit(xgrid[choices2[i,:]].reshape(-1,1), fgrid[choices2[i,:]]).predict(Xgrid)\n",
        "    axes[0].plot(xgrid, p5, color=c, alpha=0.2)\n",
        "    axes[1].plot(xgrid, p2, color=c, alpha=0.2)\n",
        "axes[0].set_title(\"samples with 5 data points\");\n",
        "axes[1].set_title(\"samples with 2 data points\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The process of learning\n",
        "\n",
        "There are challenges that occur in learning a model from data:\n",
        "\n",
        "- small samples of data\n",
        "- noise in the data\n",
        "\n",
        "Let us first ask the question: what is he process of learning from data in the absence of noise. This never really happens, but it is a way for us to understand the theory of **approximation**, and lets us build a base for understanding the learning from data with noise.\n",
        "\n",
        "Lets say we are trying to predict is a human process such as an election. Here economic and sociological factors are important, such as poverty, race and religiousness. There are historical correlations between such factors and election outcomes which we might want to incorporate into our model. An example of such a model might be:\n",
        "\n",
        "*The odds of Romney winning a county against Obama in 2012 are a function of population religiosity, race, poverty, education, and other social and economic indicators.*\n",
        "\n",
        "Our **causal** argument motivating this model here might be that religious people are more socially conservative and thus more likely to vote republican. This might not be the correct causation, but thats not entirely important for the prediction. \n",
        "\n",
        "As long as a **correlation** exists, our model is more structured than 50-50 randomness, and we can try and make a prediction. Remember of-course, our model may even be wrong (see Box's aphorism: https://en.wikipedia.org/wiki/All_models_are_wrong).\n",
        "\n",
        "We'll represent the variable being predicted, such as the probability of voting for Romney, by the letter $y$, and the **features** or **co-variates** we use as an input in this probability by the letter $x$. This $x$ could be multi-dimensional, with $x_1$ being poverty, $x_2$ being race, and so on.\n",
        "\n",
        "We then write \n",
        "\n",
        "$$ y = f(x) $$\n",
        "\n",
        "and our jobs is to take $x$ such as data from the census about race, religiousness, and so on, and $y$ as previous elections and the results of polls that pollsters come up with, and to make a predictive model for the elections. That is, we wish to estimate $f(x)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### A real simple model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To gently step feet in the modelling world, consider a very simple scenario, where the probability of voting for Romney is a function only of how religious the population in a county is. This is a model I've cooked up, and the data is fake. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let $x$ be the fraction of religious people in a county and $y$ be the fraction of people voting for Romney as a function of $x$. In other words $y_i$ is data that pollsters have taken which tells us their estimate of the fraction of people voting for Romney and $x_i$ is the fraction of religious people in county $i$. Because poll samples are finite, there is a margin of error on each data point or county $i$, but we will ignore that for now."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us assume that we have a \"population\" of 200 counties $x$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "df=pd.read_csv(\"data/religion.csv\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets suppose now that the Lord came by and told us that the points in the plot below captures $f(x)$ exactly. In other words, there is no specification error, and  God knows the generating process exactly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "x=df.rfrac.values\n",
        "f=df.promney.values\n",
        "plt.figure(figsize = (9,6))\n",
        "plt.plot(x,f,'.', alpha=0.3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice that our sampling of $x$ is not quite uniform: there are more points around $x$ of 0.7.\n",
        "\n",
        "Now, in real life we are only given a sample of points. Lets assume that out of this population of 200 points we are given a sample $\\cal{D}$ of 30 data points. Such data is called **in-sample data**. Contrastingly, the entire population of data points is also called **out-of-sample data**.\n",
        "\n",
        "Here are the indices from this sample."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "#indexes=np.sort(np.random.choice(x.shape[0], size=30, replace=False))\n",
        "dfsample = pd.read_csv(\"data/noisysample.csv\")\n",
        "dfsample.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "indexes = dfsample.i.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "samplex = x[indexes]\n",
        "samplef = f[indexes]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "collapsed": false,
        "figure_type": "w",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (9,6))\n",
        "plt.plot(x,f, 'r.', alpha=0.2, label=\"population\");\n",
        "plt.plot(samplex,samplef, 's', alpha=0.6, label=\"in-sample data $\\cal{D}$\");\n",
        "plt.legend(loc=4);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The lightly shaded squares in the right panel plot are the in-sample $\\cal{D}$ of 30 points given to us. Let us then pretend that we have forgotten the curve that the Lord gave us. Thus, all we know is the blue points on the plot on the right, and we have no clue about what the original curve was, nor do we remember the original \"population\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "That is, imagine the Lord gave us $f$ but then also gave us amnesia. Remember that such amnesia is the general case in learning, where we *do not know* the target function, but rather just have some data. Thus what we will be doing is *trying to find functions that might have generated the 30 points of data that we can see* in the hope that one of these functions might approximate $f$ well, and provide us a **predictive model** for future data. This is known as **fitting** the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Let us consider as the function we used to fit the data, a hypothesis $h$ that is a straight line. We put the subscript $1$ on the $h$ to indicate that we are fitting the data with a polynomial of order 1, or a straight line. This looks like:\n",
        "\n",
        "$$ h_1(x) = a_0 + a_1 x $$\n",
        "\n",
        "We'll call the **best fit** straight line the function $g_1(x)$. The \"best fit\" idea is this: amongst the set of all lines (i.e., all possible choices of $h_1(x)$), what is the best line $g_1(x)$ that represents the in-sample data we have? (The subscript $1$ on $g$ is chosen to indicate the best fit polynomial of degree 1, ie the line amongst lines that fits the data best).\n",
        "\n",
        "To do this, we use an algorithm, called the **learner**, which chooses functions from a hypothesis set $\\cal{H}$ and computes a cost measure or risk functional $R$ (like the sum of the squared distance over all points in the data set) for each of these functions. It then chooses the function $g$ which **minimizes** this cost measure amonst all the functions in $\\cal{H}$, and thus gives us a final hypothesis $g$ which we then use to approximate or estimate f **everywhere**, not just at the points in our data set. \n",
        "\n",
        "Here our learner is called **Polynomial Regression**, and it takes a hypothesis space $\\cal{H}_d$ of degree $d$ polynomials, minimizes the \"squared-error\" risk measure, and spits out a best-fit hypothesis $g_d$.\n",
        "\n",
        "![](images/BasicModel.png)\n",
        "\n",
        "(This diagram is based on the one in Yaser Abu-Mustafa's excellent book, Learning From Data.)\n",
        "\n",
        "The best fit $g_1(x)$ is calculated and shown in the figure below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "collapsed": false,
        "figure_type": "m",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "g1 = LinearRegression(fit_intercept=False).fit(x[indexes].reshape(-1,1),f[indexes]).predict(\n",
        "    x.reshape(-1,1))\n",
        "plt.figure(figsize = (9,6))\n",
        "plt.plot(x[indexes],f[indexes], 's', alpha=0.6, label=\"in-sample\");\n",
        "plt.plot(x,g1, alpha=0.6, label=\"$g_1$\");\n",
        "plt.legend(loc=4);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deterministic Error or Bias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice from the figure above that models in $\\cal{H}_1$, i.e., straight lines, and the best-fit straight line $g_1$ in particular, do not do a very good job of capturing the curve of  the data (and thus the underlying function $f$ that we are trying to approximate. Consider the more general case in the figure below, where a curvy $f$ is approximated by a function $g$ which just does not have the wiggling that $f$ has. \n",
        "\n",
        "![m:Bias](images/bias.png)\n",
        "\n",
        "There is always going to be an error then, in approximating $f$ by $g$. This *approximation error* is shown in the figure by the blue shaded region, and its called **bias**, or **deterministic error**. The former name comes from the fact that $g$ just does not wiggle the way $f$ does (nothing will make a straight line curve). The latter name (which I first saw used in http://www.amlbook.com/ ) comes from the notion that if you did not know the target function $f$, which is the case in most learning situations, you would have a hard time distinguishing this error from any other errors such as measurement and noise..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Going back to our model at hand, it is clear that the space of \n",
        "straight lines $\\cal{H_1}$ does not capture the curving in the data. So let us consider the more complex hypothesis space $\\cal{H_{20}}$, the set of all 20th order \n",
        "polynomials $h_{20}(x)$:\n",
        "\n",
        "$$h_{20}(x) = \\sum_{i=0}^{20} a_i x^i\\,.$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To see how a more complex hypothesis space does, lets find the best fit 20th order polynomial $g_{20}(x)$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "def polyshape(d, xgrid):\n",
        "    return PolynomialFeatures(d).fit_transform(xgrid.reshape(-1,1))\n",
        "g20 = LinearRegression(fit_intercept=False).fit(polyshape(20,x[indexes]),f[indexes]).predict(\n",
        "        polyshape(20,x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "l = LinearRegression(fit_intercept=False).fit(polyshape(20,x[indexes]),f[indexes])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "l.coef_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "collapsed": false,
        "figure_type": "m",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (9,6))\n",
        "plt.plot(x[indexes],f[indexes], 's', alpha=0.6, label=\"in-sample\");\n",
        "plt.plot(x, g20, alpha=0.6, label=\"$g_{20}$\");\n",
        "plt.legend(loc=4);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Voila! You can see the 20th order polynomial does a much better job of tracking the points, because of the wiggle room it has in making a curve \"go near or through\" all the points as opposed to a straight line, which well, cant curve. Thus it would seem that $\\cal{H}_{20}$ might be a better candidate hypothesis set from which to choose a best fit model. \n",
        "\n",
        "We can quantify this by calculating some notion of the bias for both $g_1$ and $g_{20}$. \n",
        "To do this we calculate the square of the difference between f and the g's on the population of 200 points i.e.:\n",
        "\n",
        "$$B_1(x) = (g_1(x) - f(x))^2 \\,;\\,\\, B_{20}(x) = (g_{20}(x) - f(x))^2\\,.$$ \n",
        "\n",
        "Squaring makes sure that we are calculating a positive quantity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "collapsed": false,
        "figure_type": "m",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (9,6))\n",
        "plt.plot(x, (g1-f)**2, lw=3, label=\"$B_1(x)$\")\n",
        "plt.plot(x, (g20-f)**2, lw=3,label=\"$B_{20}(x)$\");\n",
        "plt.xlabel(\"$x$\")\n",
        "plt.ylabel(\"population error\")\n",
        "plt.yscale(\"log\")\n",
        "plt.legend(loc=4);\n",
        "plt.grid()\n",
        "plt.title(\"Bias\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you can see the **bias or approximation error** is much smaller for $g_{20}$.\n",
        "\n",
        "Is $g_{20}$ the best model for this data from all possible models, both from within the ${\\cal H_{20}}$ hypothesis space and from outside? Indeed, how do we find the best fit model from the best hypothesis space? This is what **learning** is all about.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learning needs Out-of-Sample prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We write $g \\approx f$, or $g$ is the **estimand** of $f$.In statistics books you will see $g$ written as $\\hat{f}$. \n",
        "\n",
        "Why do we think that this might be a good idea? What are we really after?\n",
        "\n",
        "What we'd like to do is **make good predictions**. In the language of cost, what we are really after is to minimize the cost **out-of-sample**, on the **population** at large. But this presents us with a conundrum: *how can we minimize the risk on points we havent yet seen*?\n",
        "\n",
        "This is why we (a) minimize the risk on the set of points that we have to find $g$ and then (b) hope that once we have found our best model $g$, our risk does not particularly change out-of-sample, or when using a different set of points, and it remains low!\n",
        "\n",
        "We are, as is usual in statistics, **drawing conclusions about a population from a sample**. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### How representative is our sample?\n",
        "\n",
        "Intuitively, to do this, we need to ask ourselves, how representative is our sample? Or more precisely, how representative is our sample of our training points of the population (or for that matter the new x that we want to predict for)? \n",
        "\n",
        "We illustrate this below for our population of 200 data points and our sample of 30 data points (in red)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "collapsed": false,
        "figure_type": "m",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (9,6))\n",
        "plt.hist(x, density=True, bins=30, alpha=0.4)\n",
        "plt.plot(x[indexes], [1.0]*len(indexes),'.', alpha=0.8)\n",
        "plt.xlim([0,1]);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In our example, if we only want to use $g$, our estimand of $f$ to predict for large $x$, or more religious counties, we would need a good sampling of points $x$ closer to 1. And, similarly, the new $x$ we are using to make predictions would also need to be representative of those counties. We wont do well if we try and predict low-religiousness counties from a sample of high-religiousness ones. Or, if we do want to predict over the entire range of religiousness, our training sample better cover all $x$ well.\n",
        "\n",
        "Our sample points seem to follow our (god given) histogram well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Statement of the learning problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once we have done that, we can then intuitively say that, if we find a hypothesis $g$ that minimizes the cost or risk over the training set; this hypothesis *might* do a good job over the population that the training set was representative of, since the risk on the population ought to be similar to that on the training set, and thus small.\n",
        "\n",
        "Mathematically, we are saying that:\n",
        "\n",
        "$$\n",
        "\\begin{eqnarray*}\n",
        "A &:& R_{\\cal{D}}(g) \\,\\,smallest\\,on\\,\\cal{H}\\\\\n",
        "B &:& R_{out} (g) \\approx R_{\\cal{D}}(g)\n",
        "\\end{eqnarray*}\n",
        "$$\n",
        "\n",
        "In other words, we hope the **sample or empirical risk estimates the out of sample risk well, and thus the out of sample risk is also small**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Indeed, as we can see below, $g_{20}$ does an excellent job on the population, not just on the sample."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "collapsed": false,
        "figure_type": "m",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (9,6))\n",
        "plt.plot(x,g20, alpha=0.5, lw=2, label=\"$g_{20}$\");\n",
        "plt.plot(x,f, '.', alpha=0.5, label=\"population\");\n",
        "plt.legend(loc=4);"
      ]
    }
  ]
}